{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b23389-fd64-40c5-be2b-baadac8a2d76",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Here the data cleaning process will be performed. \n",
    "\n",
    "The goal of this process is to acquire the best possible data that could be later on fed to the ML model in order to get the best results out of it.\n",
    "\n",
    "Since we previously performed some EDA (Exploratory Data Analysis) we gathered some insights about the data, some of which are crucial for correct data cleaning process.\n",
    "\n",
    "Here are the steps, that will be performed here:\n",
    "1. Read all of the `.parquet` files with data and save it to the DataFrame\n",
    "2. Remove HTML tags (prefix & suffix)\n",
    "3. Remove prices from descriptions\n",
    "4. Remove clause about the offer (sometimes at the end)\n",
    "5. Remove links\n",
    "6. Remove emojis\n",
    "7. Remove rows with empty description\n",
    "8. Remove rows with description lenght <= 15 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b05815-c74f-426c-a62a-56a3e10859e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from demoji import replace\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe0b3a1-1cba-4864-8b49-21022d31f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695678cc-8bfc-4373-b84f-29b93f7ee9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.concat([pd.read_parquet(data_file) for data_file in glob.glob(f'{ROOT_DIR}/data/*.parquet')], ignore_index=True)\n",
    "data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1872ce0e-25d0-4a2e-8959-963ad147d0dc",
   "metadata": {},
   "source": [
    "## Function declarations\n",
    "\n",
    "First, according to the steps we are supposed to take, we will write some functions for each step of the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67dc45b5-ef84-4618-ae37-f37c543ae5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_lowercase(data: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "        A function to make a pd.Series rows lowercase\n",
    "\n",
    "        Args:\n",
    "            data: (pd.Series): pandas Series object to be processed\n",
    "        Returns:\n",
    "            pd.Series: pandas Series object with lowercase rows\n",
    "    \"\"\"\n",
    "    return data.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fe2615c-b81d-461b-9bd9-5d5472c5ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_from_html(data: pd.Series, text_to_remove: list[str], is_regex: bool = False) -> pd.Series:\n",
    "    \"\"\"\n",
    "    A function to clean prefix & suffix html tags from the pd.Series[str]\n",
    "    \n",
    "    Args:\n",
    "        data (pd.Series): pandas Series object to be processed\n",
    "        text_to_remove (list[str]): text to be removed from each record [prefix, suffix]\n",
    "        is_regex (bool): is the pattern a regex\n",
    "    Returns:\n",
    "        pd.Series: processed pandas Series object with removed text    \n",
    "    \"\"\"\n",
    "\n",
    "    def _remove_html_tags(data: pd.Series, text_to_remove: str, is_regex: bool = False) -> pd.Series:\n",
    "        \"\"\"\n",
    "        A function to remove specific text from pd.Series[str]\n",
    "\n",
    "        Args:\n",
    "            data (pd.Series): pandas Series object to be processed\n",
    "            text_to_remove (str): text to be removed from each record\n",
    "            is_regex (bool): is the pattern a regex\n",
    "        Returns:\n",
    "            pd.Series: processed pandas Series object with removed text\n",
    "        \"\"\"\n",
    "        return data.str.replace(text_to_remove, \"\", case=False, regex=is_regex)\n",
    "        \n",
    "    cleaned_prefix = _remove_html_tags(data, text_to_remove[0], is_regex)\n",
    "    cleaned_data = _remove_html_tags(cleaned_prefix, text_to_remove[1], is_regex)\n",
    "    \n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "be1c3d33-0672-40cd-b996-96a1104bab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_with_regex(text: str, regex_pattern: str) -> str:\n",
    "    \"\"\"\n",
    "    A function to remove some pattern from text with the use of regex\n",
    "\n",
    "    Args:\n",
    "        text (str): text from which the contents should be removed\n",
    "        pattern (str): pattern to be used\n",
    "    Returns:\n",
    "        str: text without the things specified in the parrern\n",
    "    \"\"\"\n",
    "    return re.sub(regex_pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "167e6873-fd85-44b5-829e-e1277e7339e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows_without_description(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A function to remove records without description\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): data frame to process\n",
    "    Returns:\n",
    "        pd.DataFrame: processed data frame object\n",
    "    \"\"\"\n",
    "    rows_to_keep = []\n",
    "    for index, row in data.iterrows():\n",
    "        if not re.search(REGEX_EMPTY, row['description'], flags=re.IGNORECASE) and len(row['description'].split()) > 0:\n",
    "            rows_to_keep.append(index)\n",
    "    \n",
    "    return data.loc[rows_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "914b87bf-e496-4dac-a467-b8db91cb3d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_descriptions(data: pd.Series, threshold: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    A function to remove short descriptions\n",
    "    \n",
    "    Args:\n",
    "        data (pd.Series): column with text\n",
    "        threshold (int): length in words\n",
    "    Returns:\n",
    "        pd.Series: cleaned data\n",
    "    \"\"\"\n",
    "    rows_to_keep = []\n",
    "    for index, row in data.iterrows():\n",
    "        if len(row['description'].split()) >= threshold:\n",
    "            rows_to_keep.append(index)\n",
    "\n",
    "    return data.loc[rows_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "eeabbec4-2951-4b63-9130-b4c925470263",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_PREFIX = r'^.*?\\n.*?\\n'\n",
    "HTML_SUFFIX = r'\\n.*?\\n.*?\\n.*?\\n.*?$'\n",
    "\n",
    "REGEX_PRICE = r'.*(?:cena|brutto|netto|23%|rabat|pln|eur|usd|zł).*$'\n",
    "REGEX_CLAUSE = r'ogłoszenie\\s+nie\\s+stanowi\\s+oferty.*$'\n",
    "REGEX_LINK = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
    "REGEX_EMPTY = r'brak opisu pojazdu\\. aby uzyskać więcej informacji, skontaktuj się ze sprzedawcą\\.'\n",
    "\n",
    "WORD_COUNT_THRESHOLD = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2eee9235-754f-44fd-bc99-93becd628449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Lowercase\n",
    "data['description'] = make_text_lowercase(data['description'])\n",
    "\n",
    "# 2. Html clean\n",
    "data['description'] = clean_data_from_html(data['description'], [HTML_PREFIX, HTML_SUFFIX], True)\n",
    "\n",
    "# 3. Remove prices from description\n",
    "data['description'] = data['description'].apply(lambda x: remove_with_regex(x, REGEX_PRICE))\n",
    "\n",
    "# 4. Remove the clause 'Ogloszenie nie stanowi oferty ...'\n",
    "data['description'] = data['description'].apply(lambda x: remove_with_regex(x, REGEX_CLAUSE))\n",
    "\n",
    "# 5. Remove links\n",
    "data['description'] = data['description'].apply(lambda x: remove_with_regex(x, REGEX_LINK))\n",
    "\n",
    "# 6. Remove emojis\n",
    "data['description'] = data['description'].apply(replace)\n",
    "\n",
    "# 7. Remove rows with no description\n",
    "data = remove_rows_without_description(data)\n",
    "\n",
    "# 8. Remove records with description shorter than 15 words\n",
    "data = remove_short_descriptions(data, WORD_COUNT_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "a6535e8e-1749-45b8-84f0-e70bfa3ee187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 25537 records\n",
      "After:  13662 records\n"
     ]
    }
   ],
   "source": [
    "print(f'Before: {raw_data.shape[0]} records')\n",
    "print(f'After:  {data.shape[0]} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "a30bd8c4-49a5-4595-82c9-c0877ff1ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"cleaned_data.csv\", sep=';', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c63c9-465a-4538-9e76-744dc221c683",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This will be done by using **transformers** library from *huggingface*\n",
    "\n",
    "For this you will need:\n",
    "- protobuf (pip install protobuf)\n",
    "- transformers (pip install transformers)\n",
    "- pytorch or tensorflow (pip install torch torchvision)\n",
    "- sacremoses (pip install sacremoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0484ee3d-f7d0-4135-8ba2-ad3550cd7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dabfde27-00e2-4871-a8f5-828ecc581185",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"allegro/herbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5797595d-4c87-4167-9feb-a049c9a08b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./cleaned_data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e287a1c5-e024-4d25-b4f7-ab6557b32255",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\n",
    "    data['description'].tolist(),\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt'    # change this to tf for tensorflow\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ab45f11-c05a-4923-af25-a9ac1cf64fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
