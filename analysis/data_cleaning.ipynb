{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b23389-fd64-40c5-be2b-baadac8a2d76",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Here the data cleaning process will be performed. \n",
    "\n",
    "The goal of this process is to acquire the best possible data that could be later on fed to the ML model in order to get the best results out of it.\n",
    "\n",
    "Since we previously performed some EDA (Exploratory Data Analysis) we gathered some insights about the data, some of which are crucial for correct data cleaning process.\n",
    "\n",
    "Here are the steps, that will be performed here:\n",
    "1. Read all of the `.parquet` files with data and save it to the DataFrame\n",
    "2. Remove HTML tags (prefix & suffix)\n",
    "3. Remove prices from descriptions\n",
    "4. Remove clause about the offer (sometimes at the end)\n",
    "5. Remove links\n",
    "6. Remove emojis\n",
    "7. Remove rows with empty description\n",
    "8. Remove rows with description lenght <= 15 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b05815-c74f-426c-a62a-56a3e10859e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:56:38.365655Z",
     "start_time": "2025-11-26T09:56:34.967074Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from demoji import replace\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe0b3a1-1cba-4864-8b49-21022d31f121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:56:38.371864Z",
     "start_time": "2025-11-26T09:56:38.370049Z"
    }
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "695678cc-8bfc-4373-b84f-29b93f7ee9ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:56:39.600274Z",
     "start_time": "2025-11-26T09:56:38.377506Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_data = pd.concat([pd.read_parquet(data_file) for data_file in glob.glob(f'{ROOT_DIR}/data/raw/*.parquet')], ignore_index=True)\n",
    "data = raw_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1872ce0e-25d0-4a2e-8959-963ad147d0dc",
   "metadata": {},
   "source": [
    "## Function declarations\n",
    "\n",
    "First, according to the steps we are supposed to take, we will write some functions for each step of the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67dc45b5-ef84-4618-ae37-f37c543ae5b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:56:39.623676Z",
     "start_time": "2025-11-26T09:56:39.621788Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_text_lowercase(data: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "        A function to make a pd.Series rows lowercase\n",
    "\n",
    "        Args:\n",
    "            data: (pd.Series): pandas Series object to be processed\n",
    "        Returns:\n",
    "            pd.Series: pandas Series object with lowercase rows\n",
    "    \"\"\"\n",
    "    return data.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fe2615c-b81d-461b-9bd9-5d5472c5ef2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:56:39.629690Z",
     "start_time": "2025-11-26T09:56:39.627550Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_data_from_html(data: pd.Series, text_to_remove: list[str], is_regex: bool = False) -> pd.Series:\n",
    "    \"\"\"\n",
    "    A function to clean prefix & suffix html tags from the pd.Series[str]\n",
    "    \n",
    "    Args:\n",
    "        data (pd.Series): pandas Series object to be processed\n",
    "        text_to_remove (list[str]): text to be removed from each record [prefix, suffix]\n",
    "        is_regex (bool): is the pattern a regex\n",
    "    Returns:\n",
    "        pd.Series: processed pandas Series object with removed text    \n",
    "    \"\"\"\n",
    "\n",
    "    def _remove_html_tags(data: pd.Series, text_to_remove: str, is_regex: bool = False) -> pd.Series:\n",
    "        \"\"\"\n",
    "        A function to remove specific text from pd.Series[str]\n",
    "\n",
    "        Args:\n",
    "            data (pd.Series): pandas Series object to be processed\n",
    "            text_to_remove (str): text to be removed from each record\n",
    "            is_regex (bool): is the pattern a regex\n",
    "        Returns:\n",
    "            pd.Series: processed pandas Series object with removed text\n",
    "        \"\"\"\n",
    "        return data.str.replace(text_to_remove, \"\", case=False, regex=is_regex)\n",
    "        \n",
    "    cleaned_prefix = _remove_html_tags(data, text_to_remove[0], is_regex)\n",
    "    cleaned_data = _remove_html_tags(cleaned_prefix, text_to_remove[1], is_regex)\n",
    "    \n",
    "    return cleaned_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be1c3d33-0672-40cd-b996-96a1104bab5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:56:39.634510Z",
     "start_time": "2025-11-26T09:56:39.633Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_with_regex(text: str, regex_pattern: str) -> str:\n",
    "    \"\"\"\n",
    "    A function to remove some pattern from text with the use of regex\n",
    "\n",
    "    Args:\n",
    "        text (str): text from which the contents should be removed\n",
    "        pattern (str): pattern to be used\n",
    "    Returns:\n",
    "        str: text without the things specified in the parrern\n",
    "    \"\"\"\n",
    "    return re.sub(regex_pattern, '', text, flags=re.IGNORECASE | re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "167e6873-fd85-44b5-829e-e1277e7339e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:56:39.639519Z",
     "start_time": "2025-11-26T09:56:39.637644Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_rows_without_description(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A function to remove records without description\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): data frame to process\n",
    "    Returns:\n",
    "        pd.DataFrame: processed data frame object\n",
    "    \"\"\"\n",
    "    rows_to_keep = []\n",
    "    for index, row in data.iterrows():\n",
    "        if not re.search(REGEX_EMPTY, row['description'], flags=re.IGNORECASE) and len(row['description'].split()) > 0:\n",
    "            rows_to_keep.append(index)\n",
    "    \n",
    "    return data.loc[rows_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "914b87bf-e496-4dac-a467-b8db91cb3d78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:56:39.644560Z",
     "start_time": "2025-11-26T09:56:39.642711Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_short_descriptions(data: pd.Series, threshold: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    A function to remove short descriptions\n",
    "    \n",
    "    Args:\n",
    "        data (pd.Series): column with text\n",
    "        threshold (int): length in words\n",
    "    Returns:\n",
    "        pd.Series: cleaned data\n",
    "    \"\"\"\n",
    "    rows_to_keep = []\n",
    "    for index, row in data.iterrows():\n",
    "        if len(row['description'].split()) >= threshold:\n",
    "            rows_to_keep.append(index)\n",
    "\n",
    "    return data.loc[rows_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeabbec4-2951-4b63-9130-b4c925470263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T09:56:39.649588Z",
     "start_time": "2025-11-26T09:56:39.647986Z"
    }
   },
   "outputs": [],
   "source": [
    "HTML_PREFIX = r'^.*?\\n.*?\\n'\n",
    "HTML_SUFFIX = r'\\n.*?\\n.*?\\n.*?\\n.*?$'\n",
    "\n",
    "REGEX_PRICE = r'.*(?:cena|brutto|netto|23%|rabat|pln|eur|usd|zł).*$'\n",
    "REGEX_CLAUSE = r'ogłoszenie\\s+nie\\s+stanowi\\s+oferty.*$'\n",
    "REGEX_LINK = r'https?://[^\\s]+|www\\.[^\\s]+'\n",
    "REGEX_EMPTY = r'brak opisu pojazdu\\. aby uzyskać więcej informacji, skontaktuj się ze sprzedawcą\\.'\n",
    "\n",
    "WORD_COUNT_THRESHOLD = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eee9235-754f-44fd-bc99-93becd628449",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T10:00:05.306480Z",
     "start_time": "2025-11-26T09:56:39.653288Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Lowercase\n",
    "data['description'] = make_text_lowercase(data['description'])\n",
    "\n",
    "# 2. Html clean\n",
    "data['description'] = clean_data_from_html(data['description'], [HTML_PREFIX, HTML_SUFFIX], True)\n",
    "\n",
    "# 3. Remove prices from description\n",
    "data['description'] = data['description'].apply(lambda x: remove_with_regex(x, REGEX_PRICE))\n",
    "\n",
    "# 4. Remove the clause 'Ogloszenie nie stanowi oferty ...'\n",
    "data['description'] = data['description'].apply(lambda x: remove_with_regex(x, REGEX_CLAUSE))\n",
    "\n",
    "# 5. Remove links\n",
    "data['description'] = data['description'].apply(lambda x: remove_with_regex(x, REGEX_LINK))\n",
    "\n",
    "# 6. Remove emojis\n",
    "data['description'] = data['description'].apply(replace)\n",
    "\n",
    "# 7. Remove rows with no description\n",
    "data = remove_rows_without_description(data)\n",
    "\n",
    "# 8. Remove records with description shorter than 15 words\n",
    "data = remove_short_descriptions(data, WORD_COUNT_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6535e8e-1749-45b8-84f0-e70bfa3ee187",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T10:00:05.338640Z",
     "start_time": "2025-11-26T10:00:05.336633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 25537 records\n",
      "After:  23752 records\n"
     ]
    }
   ],
   "source": [
    "print(f'Before: {raw_data.shape[0]} records')\n",
    "print(f'After:  {data.shape[0]} records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a30bd8c4-49a5-4595-82c9-c0877ff1ac0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T10:00:06.256837Z",
     "start_time": "2025-11-26T10:00:05.350211Z"
    }
   },
   "outputs": [],
   "source": [
    "data.to_csv(f\"{ROOT_DIR}/data/processed/cleaned_data.csv\", sep=';', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c63c9-465a-4538-9e76-744dc221c683",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This will be done by using **transformers** library from *huggingface*\n",
    "\n",
    "For this you will need:\n",
    "- protobuf (pip install protobuf)\n",
    "- transformers (pip install transformers)\n",
    "- pytorch or tensorflow (pip install torch torchvision)\n",
    "- sacremoses (pip install sacremoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0484ee3d-f7d0-4135-8ba2-ad3550cd7006",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T10:00:07.140379Z",
     "start_time": "2025-11-26T10:00:06.270469Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dabfde27-00e2-4871-a8f5-828ecc581185",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-26T10:00:09.973239Z",
     "start_time": "2025-11-26T10:00:07.153524Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ac96cbaf86403cad2c95aac849b9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe72c5543c642299c7f38c8251bbe01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6960ceb0dced4769b5a2098a3de7dedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e21a73f1754737845e471a38b97805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4908f1342ab8405581f577f2bfa5e311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/studies_repos/ADM_project/.venv/lib/python3.13/site-packages/transformers/models/herbert/tokenization_herbert.py:323\u001b[39m, in \u001b[36mHerbertTokenizer.__init__\u001b[39m\u001b[34m(self, vocab_file, merges_file, tokenizer_file, cls_token, unk_token, pad_token, mask_token, sep_token, bos_token, do_lowercase_and_remove_accent, additional_special_tokens, lang2id, id2lang, **kwargs)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msacremoses\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sacremoses'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/studies_repos/ADM_project/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2359\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/studies_repos/ADM_project/.venv/lib/python3.13/site-packages/transformers/models/herbert/tokenization_herbert.py:325\u001b[39m, in \u001b[36mHerbertTokenizer.__init__\u001b[39m\u001b[34m(self, vocab_file, merges_file, tokenizer_file, cls_token, unk_token, pad_token, mask_token, sep_token, bos_token, do_lowercase_and_remove_accent, additional_special_tokens, lang2id, id2lang, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    326\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou need to install sacremoses to use HerbertTokenizer. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSee https://pypi.org/project/sacremoses/ for installation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    328\u001b[39m     )\n\u001b[32m    330\u001b[39m \u001b[38;5;28mself\u001b[39m.sm = sacremoses\n",
      "\u001b[31mImportError\u001b[39m: You need to install sacremoses to use HerbertTokenizer. See https://pypi.org/project/sacremoses/ for installation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mallegro/herbert-base-cased\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/studies_repos/ADM_project/.venv/lib/python3.13/site-packages/transformers/models/auto/tokenization_auto.py:1156\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1153\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1154\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist or is not currently imported.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1155\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[32m   1159\u001b[39m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[32m   1160\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/studies_repos/ADM_project/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2113\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2110\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2111\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/studies_repos/ADM_project/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2151\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2148\u001b[39m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[32m   2149\u001b[39m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[32m   2150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (from_slow \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tokenizer_file) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.slow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[32m-> \u001b[39m\u001b[32m2151\u001b[39m     slow_tokenizer = \u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mslow_tokenizer_class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2155\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2159\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2160\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2161\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2163\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/studies_repos/ADM_project/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2360\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2359\u001b[39m     tokenizer = \u001b[38;5;28mcls\u001b[39m(*init_inputs, **init_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mimport_protobuf_decode_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2361\u001b[39m     logger.info(\n\u001b[32m   2362\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2363\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2364\u001b[39m     )\n\u001b[32m   2365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/studies_repos/ADM_project/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:88\u001b[39m, in \u001b[36mimport_protobuf_decode_error\u001b[39m\u001b[34m(error_message)\u001b[39m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DecodeError\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PROTOBUF_IMPORT_ERROR.format(error_message))\n",
      "\u001b[31mImportError\u001b[39m: \n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"allegro/herbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5797595d-4c87-4167-9feb-a049c9a08b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'{ROOT_DIR}/data/processed/cleaned_data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e287a1c5-e024-4d25-b4f7-ab6557b32255",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\n",
    "    data['description'].tolist(),\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt'    # change this to tf for tensorflow\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ab45f11-c05a-4923-af25-a9ac1cf64fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
